{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71aa1807-dc2c-435d-85fe-ad67c82f0726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDE80 Databricks Project Setup & Pipeline Guide: alldatatech_project\n",
    "\n",
    "This document provides step-by-step instructions for setting up the environment, ingesting initial data, and configuring the monthly extraction pipeline within **Databricks** using **Unity Catalog**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ⚙️ Initial Environment Setup & Data Ingestion\n",
    "\n",
    "The steps below are executed using the **`alldatatech_project Setup and Data Ingestion.ipynb`** notebook.\n",
    "\n",
    "### 1.1 Pre-Requisite Actions\n",
    "\n",
    "1.  **Download Files:** Download all necessary project files from the **GitHub** repository to your local machine.\n",
    "2.  **Import Notebook:** Import the notebook named **`alldatatech_project Setup and Data Ingestion.ipynb`** into your Databricks workspace.\n",
    "\n",
    "### 1.2 Notebook Execution Steps\n",
    "\n",
    "Execute the query cells in the imported setup notebook **sequentially** to establish the necessary database objects and prepare for data ingestion.\n",
    "\n",
    "| Step # | Action | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **1** | Creating a New Spark **Catalog** | Defines the top-level namespace (e.g., `alldatatech_project`) in Unity Catalog. |\n",
    "| **2** | Creating a New Spark **Schema** (Database) | Creates a schema within the catalog (e.g., `sales_reporting`) to organize tables. |\n",
    "| **3** | Creating the **`regional_sales_data`** Delta Table | Defines the target Delta Lake table for the ingested data. |\n",
    "| **4** | Creating a Unity Catalog **Volume** | Sets up a Unity Catalog **Volume** to manage access to cloud storage for raw files (e.g., CSVs). |\n",
    "| **5** | **Upload Sample Data** (Manual) | **MANUAL ACTION:** Upload the file **`Regional_sales_dataset.csv`** to the following Volume path: **`alldatatech_project.sales_reporting.sales`**. |\n",
    "| **6** | PySpark: Ingesting CSV Data | Reads the uploaded CSV file from the Volume and loads it into the **`regional_sales_data`** Delta table. |\n",
    "| **7** | Viewing All Data | Validates successful ingestion by displaying the contents of the target table. |\n",
    "| **8** | Creating a Subdirectory | Creates a new subdirectory within the Volume path (e.g., for archiving processed files). |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. \uD83D\uDE80 Configuring the Extraction Pipeline\n",
    "\n",
    "This section configures the monthly extraction process using a Databricks Job pipeline.\n",
    "\n",
    "### 2.1 Job Creation via YAML\n",
    "\n",
    "1.  **Navigate to Jobs:** Go to **Workflows** (or Jobs & Pipelines) in the Databricks workspace sidebar.\n",
    "2.  **Create Job:** Click **\"Create Job\"**.\n",
    "3.  **Edit as YAML:** Switch the job creation interface to the **\"Edit as YAML\"** view.\n",
    "4.  **Paste YAML Code:** Copy the entire content of the **`wf_regional_sales_extract.yml`** file and paste it into the YAML editor.\n",
    "5.  **Save:** Save the job configuration. This creates the pipeline named **`wf_regional_sales_extract`**.\n",
    "\n",
    "### 2.2 Notebook Preparation\n",
    "\n",
    "1.  **Import Extraction Notebook:** Import the notebook named **`Sales_Extract.ipynb`** into your Databricks workspace.\n",
    "2.  **Update Pipeline Path:**\n",
    "    * Open the newly created job **`wf_regional_sales_extract`**.\n",
    "    * Inspect the notebook task within the job.\n",
    "    * **Update the Notebook Path** in the job task definition to point to the exact location of the imported **`Extract.ipynb`** file in your workspace.\n",
    "\n",
    "### 2.3 Running the Pipeline\n",
    "\n",
    "1.  **Execute Job:** Open the **`wf_regional_sales_extract`** job.\n",
    "2.  Click **\"Run now\"** to execute the pipeline and initiate the data extraction process.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Artifacts Summary\n",
    "\n",
    "| Artifact | Type | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **`alldatatech_project Setup and Data Ingestion.ipynb`** | Notebook | One-time setup of UC Catalog, Schema, Volume, and Delta Table. |\n",
    "| **`Regional_sales_dataset.csv`** | Raw Data File | Initial data source uploaded to the Volume. |\n",
    "| **`wf_regional_sales_extract.yml`** | YAML Definition | Configuration file for the Databricks Job (Pipeline). |\n",
    "| **`Sales_Extract.ipynb`** | Notebook | Contains the primary ETL/extraction logic run by the job. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3fa5e0-4143-4dbd-b997-270bcc304a50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a New Spark Catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG alldatatech_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe1f612-efca-4cd0-ac37-7368046bdb61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a New Spark Schema (Database)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA alldatatech_project.sales_reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf81237-4dfd-49ce-a9b7-0aa28c86b1e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the enrollment_status_log Delta Table"
    }
   },
   "outputs": [],
   "source": [
    " %sql\n",
    "CREATE TABLE alldatatech_project.sales_reporting.regional_sales_data (\n",
    "  sale_id BIGINT COMMENT 'Unique identifier for the sales transaction',\n",
    "  sale_date DATE COMMENT 'The date the sale occurred (used for partitioning)',\n",
    "  region_name STRING COMMENT 'The geographic region where the sale was made (e.g., North, South, East, West)',\n",
    "  country_code STRING COMMENT 'Two-letter ISO country code (e.g., US, CA, UK)',\n",
    "  product_sku STRING COMMENT 'Stock Keeping Unit (SKU) identifier for the product sold',\n",
    "  product_name STRING COMMENT 'The full name of the product sold',\n",
    "  quantity INT COMMENT 'The number of units sold in this transaction',\n",
    "  unit_price DECIMAL(10, 2) COMMENT 'The price per unit at the time of sale',\n",
    "  total_revenue DECIMAL(12, 2) COMMENT 'The calculated total revenue for this line item (quantity * unit_price)',\n",
    "  dealer_id STRING COMMENT 'Identifier for the dealer or store where the sale originated',  \n",
    "  customer_id STRING COMMENT 'Unique identifier for the purchasing customer',\n",
    "  customer_segment STRING COMMENT 'Categorization of the customer (e.g., B2B, B2C, Premium)',  \n",
    "  discount_amount DECIMAL(10, 2) COMMENT 'Total discount applied to this transaction line item',\n",
    "  cost_of_goods_sold DECIMAL(10, 2) COMMENT 'The direct cost to the company for the units sold',\n",
    "  gross_profit DECIMAL(12, 2) COMMENT 'Calculated gross profit: total_revenue - cost_of_goods_sold',\n",
    "  payment_method STRING COMMENT 'The method of payment used (e.g., Credit Card, Cash, Invoice)',  \n",
    "  shipment_status STRING COMMENT 'The current status of the order fulfillment (e.g., Shipped, Delivered, Pending)',\n",
    "  shipping_fee DECIMAL(8, 2) COMMENT 'The fee charged for shipping this line item',  \n",
    "  sales_channel STRING COMMENT 'The primary channel of the sale (e.g., Web, Retail, Field Sales, Tele)',\n",
    "  campaign_id STRING COMMENT 'The ID of the marketing campaign that influenced this sale, if any',\n",
    "  sales_rep_id STRING COMMENT 'The ID of the sales representative or employee responsible for the sale',\n",
    "  created_ts TIMESTAMP COMMENT 'The timestamp when this record was inserted into the table'\n",
    ")\n",
    "USING delta\n",
    "PARTITIONED BY (sale_date)\n",
    "COMMENT 'Detailed transactional data for regional sales reporting and analysis, including marketing and personnel attribution.'\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2',\n",
    "  'external.table.purge' = 'true',\n",
    "  'delta.logRetentionDuration' = 'INTERVAL 30 DAYS'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38089da0-9c95-4de7-afce-118a3963ca06",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Unity Catalog Volume"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE VOLUME alldatatech_project.sales_reporting.sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "897b53b1-f211-445e-a1e5-8f35fd5e52be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### MANUAL ACTION: Upload the file Regional_sales_dataset.csv to the following Volume path: /Volumes/alldatatech_project/sales_reporting/sales/Regional_sales_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6867d23c-dd65-447a-b931-5c3b9ffa20e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark: Ingesting CSV Data from Volume into Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"nullValue\", \"null\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"/Volumes/alldatatech_project/sales_reporting/sales/Regional_sales_dataset.csv\") \n",
    "display(df)\n",
    "# print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Insert data into existing table\n",
    "df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .insertInto(\"alldatatech_project.sales_reporting.regional_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb73682-8d1e-4379-b699-a516f4b67ae8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing All Data inenrollment_status_log Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from alldatatech_project.sales_reporting.regional_sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce35dca-0b84-4130-8621-14ee9b1c1af4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Subdirectory within the Volume"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/alldatatech_project/sales_reporting/sales/reporting\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7246585666521658,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "alldatatech_project Setup and Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}